<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="LumberChunker: Efficient Text Chunking with Late Merging">
  <meta name="keywords" content="LumberChunker, RAG, Text Chunking, LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>LumberChunker: Long-Form Narrative Document Segmentation</title>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,500,700">

  <!-- Bulma CSS -->
  <link rel="stylesheet" href="../disco/static/css/bulma.min.css">
  <link rel="stylesheet" href="../disco/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../disco/static/css/bulma-slider.min.css">
  
  <!-- FontAwesome & Academicons -->
  <link rel="stylesheet" href="../disco/static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <!-- Your Custom CSS -->
  <link rel="stylesheet" href="../disco/static/css/index.css">
  
  <!-- Favicon -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>LC</text></svg>">

  <!-- jQuery -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  
  <!-- Bootstrap CSS AFTER Bulma so that the navbar styles take precedence -->
  <link rel="stylesheet" 
        href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css"
        integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA=="
        crossorigin="anonymous" />

  <!-- Bootstrap JS (with Popper.js) -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
          integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
          crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js"
          integrity="sha512-XKa9Hemdy1Ui3KSGJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg=="
          crossorigin="anonymous"></script>
  
  <!-- FontAwesome, Bulma Carousel, etc. -->
  <script defer src="../disco/static/js/fontawesome.all.min.js"></script>
  <script src="../disco/static/js/bulma-carousel.min.js"></script>
  <script src="../disco/static/js/bulma-slider.min.js"></script>
  <script src="../disco/static/js/index.js"></script>

  <!-- d3 -->
  <script src="https://d3js.org/d3.v7.min.js"></script>

  <style>
    /* Slightly increase the base font size */
    body {
        font-size: 20px;
        color: #424242 !important;
    }

    p {
      margin-bottom: 12px !important;
    }

    /* For mobile devices: change text alignment to left */
    @media screen and (max-width: 1000px) {
      .content.has-text-justified {
        text-align: left !important;
      }
    }

    .title.is-3{
      color: #3e3e3e !important;
      font-weight: bold;
      margin-top: -2rem;
    }

    .title.is-4 {
      color: #686868 !important;
      font-weight: bold;
      margin-bottom: 0.5rem;
    }

    .image-wrapper{
      justify-content: center;
      align-items: center;
      margin-bottom: 20px;
    }

    @media screen and (max-width: 1000px) {
      .image-wrapper {
        width: 100% !important;
      }
      .image-wrapper img {
        width: 90% !important;
      }
    }

    /* Quiz/Interactive Section Styling */
    .quiz-section {
      background-color: #f8f9fa;
      padding: 2rem;
      border-radius: 10px;
      margin: 2rem 0;
    }

    .quiz-iframe-container {
      width: 100%;
      height: 800px;
      border: 2px solid #e0e0e0;
      border-radius: 10px;
      overflow: hidden;
      background: white;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }

    .quiz-iframe-container iframe {
      width: 100%;
      height: 100%;
      border: none;
    }

    /* Custom Tab Styling */
    .custom-tabs {
      margin-bottom: 1.5rem !important;
      border-bottom: 1px solid #e0e0e0 !important;
      font-family: 'Roboto', sans-serif !important;
    }

    .custom-tabs ul {
      display: flex !important;
      justify-content: center !important;
      list-style: none !important;
      padding: 0 !important;
      margin: 0 !important;
    }

    .custom-tabs ul li {
      padding: 0.75rem 1.5rem !important;
      margin: 0 0.5rem !important;
      cursor: pointer !important;
      position: relative !important;
      font-size: 1.1rem !important;
      color: #9e9e9e !important;
      transition: color 0.25s ease !important;
    }

    .custom-tabs ul li:not(.is-active):hover {
      color: rgb(47, 47, 47) !important;
    }

    .custom-tabs ul li.is-active {
      color: #464646 !important;
      font-weight: 600 !important;
    }

    .custom-tabs ul li.is-active::after {
      content: '' !important;
      position: absolute !important;
      left: 0 !important;
      right: 0 !important;
      bottom: -1px !important;
      height: 3px !important;
      background-color: #464646 !important;
      border-radius: 2px !important;
    }

    /* Card-like Table Container */
    .table-card {
      margin: 0 auto !important;
      max-width: 100% !important;
    }

    table.table {
      width: 100% !important;
      font-family: 'Roboto', sans-serif !important;
      border-collapse: collapse !important;
    }

    table.table caption {
      caption-side: top !important;
      text-align: center !important;
      font-weight: 600 !important;
      margin-bottom: 0.75rem !important;
      font-size: 1.15rem !important;
      color: #464646 !important;
    }

    table.table th,
    table.table td {
      padding: 0.75rem !important;
      border: 1px solid #f0f0f0 !important;
      text-align: center !important;
      font-size: clamp(0.65rem, 2vw, 1rem) !important;
      color: #464646 !important;
    }

    table.table th {
      background: #fafafa !important;
      vertical-align: middle !important;
    }

    .table-card table.table th:first-child,
    .table-card table.table td:first-child {
      white-space: nowrap !important;
      text-align: left !important;
    }

    /* Algorithm box styling */
    .algorithm-box {
      background-color: #f5f5f5;
      border-left: 4px solid #626161;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-radius: 4px;
    }

    .algorithm-box code {
      background-color: #fff;
      padding: 0.2rem 0.4rem;
      border-radius: 3px;
      font-size: 0.9em;
    }

  </style>
</head>
<body>

  <!-- Begin Bulma-Based Page Content -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              LumberChunker: Long-Form Narrative Document Segmentation
            </h1>
            <p style="font-size: 1.1rem; color: #666; margin-bottom: 1rem;">
              Blog created by Raymond Jiang
            </p>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#">André V. Duarte</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="#">João Marques</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="#">Miguel Graça</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="#">Miguel Freire</a><sup>2</sup>
              </span>
              <span class="author-block">
                <a href="#">Lei Li</a><sup>3</sup>
              </span>
              <span class="author-block">
                <a href="#">Arlindo Oliveira</a><sup>1</sup>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>INESC-ID / Instituto Superior Técnico, <sup>2</sup>NeuralShift AI, <sup>3</sup>Carnegie Mellon University</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.17526" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link -->
                <span class="link-block">
                  <a href="https://github.com/joaodsmarques/LumberChunker" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <div class="image-wrapper" style="display: flex; justify-content: center; align-items: center;">
            <img src="LumberChunker_pipeline.png" alt="LumberChunker Pipeline" style="max-width: 80%; border-radius: 8px;">
          </div>
          <p style="margin-top: 1rem;">
            We present LumberChunker, a method for semantically segmenting long-form narrative documents that achieves state-of-the-art retrieval performance while requiring significantly fewer embedding computations than existing approaches.
          </p>
        </div>
      </div>
    </div>
  </section>

  
<!-- Introduction & Key Idea -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-left">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Long-form narratives (novels, memoirs, transcripts) don’t break cleanly at fixed token counts. If we split them poorly, Retrieval-Augmented Generation (RAG) pipelines surface the wrong passages and LLMs start guessing. Structure-only chunkers (fixed tokens, paragraphs) are fast but blind to scene and topic flow; purely similarity-driven heuristics can fragment dialogue, miss coreference, and drift as context grows.
          </p>
          <p>
            <span style="font-weight: 600;">So how do we preserve the story’s flow and still keep chunking practical?</span>
          </p>

          <h2 class="title is-4">The Key Idea</h2>
          <p>
            Treat segmentation as <em>boundary finding</em>. LumberChunker reads a rolling window of consecutive paragraphs (up to a token count <code>θ</code>) and asks an LLM to return the <strong>earliest paragraph where the content clearly shifts</strong> from what came before. That paragraph marks a boundary; the next chunk starts there. Repeat until the document ends.
          </p>
          <p>
            This simple prompt design yields reliable, human-like cuts because:
          </p>
          <ul>
            <li>
              <strong>Low false positives when no shift exists.</strong> Without a genuine topic/scene turn, the model rarely flags a boundary, so chunks don’t fracture mid-thought.
            </li>
            <li>
              <strong>High hit-rate when a shift does exist.</strong> When the narrative pivots (new scene, entity, objective), the model consistently identifies the earliest turning point, preserving coherence.
            </li>
          </ul>
          <p>
            In practice, sweeping the window size shows a sweet spot around <strong>θ ≈ 550 tokens</strong>: enough context to recognize transitions, but not so much that the signal gets diluted.
          </p>

          <p>
            Want to get a feel for it? Try the interactive demo below and place boundaries yourself, then compare with LumberChunker’s cuts. Can you spot the same turns the model does? 👀
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


  <!-- Interactive Quiz Section -->
  <section class="section" id="quiz" style="margin-top: -2rem; margin-bottom: -2.5rem;">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">
        Interactive Text Segmentation Challenge
      </h2>
      <div class="quiz-section">
        <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
          <p>
            Below is an interactive demonstration where you can practice document segmentation. Place breaks between sentences to create semantically coherent chunks.
          </p>
          <p>
            <strong>Instructions:</strong> Click on the dividing line between sentences to insert a chunk boundary. Sentences between boundaries are automatically grouped and color-coded. When finished, compare your segmentation with LumberChunker's output to evaluate how your intuition aligns with semantic similarity metrics.
          </p>
        </div>
        <div class="quiz-iframe-container">
          <iframe src="quiz.html" title="Interactive Text Chunking Quiz"></iframe>
        </div>
      </div>
    </div>
  </section>


  <!-- Method Section (Updated to LumberChunker style) -->
  <section class="section" id="method">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-left">
        <div class="column is-four-fifths">
          <h2 class="title is-3">The LumberChunker Method</h2>
          <div class="content has-text-justified">

            <p>
              LumberChunker treats document segmentation as a <em>boundary-finding</em> problem. Instead of cutting by fixed tokens or paragraphs, we ask an LLM to read a rolling window of consecutive paragraphs and return the <strong>first paragraph where the content clearly shifts</strong>. That cut becomes the end of the current chunk and the start of the next, which yields variable-length, semantically coherent segments that track narrative flow.
            </p>

            <h3 class="title is-4">1) Paragraphize</h3>
            <p>
              Cleanly split the book into paragraphs and assign stable IDs (<code>p1, p2, …</code>). This preserves the document’s natural discourse units and gives us safe candidate boundaries.
            </p>

            <div class="algorithm-box">
              <strong>Step 1: Paragraph Extraction</strong><br>
              <code>paragraphs = extract_paragraphs(document)</code><br>
              Output: <code>[(id: p1, text), (id: p2, text), ...]</code>
            </div>

            <h3 class="title is-4">2) Grow a window</h3>
            <p>
              Build a rolling group <code>G<sub>i</sub></code> by appending paragraphs until the group’s length reaches a token budget <code>θ</code>. This provides enough context for the model to judge when a topic/scene actually shifts.
            </p>

            <div class="algorithm-box">
              <strong>Step 2: Group Builder</strong><br>
              <code>G = []</code><br>
              <code>while tokens(G) &lt;= θ: G.append(next_paragraph())</code><br>
              Output: contextual span <code>G<sub>i</sub></code> with IDs and text
            </div>

            <h3 class="title is-4">3) Ask the LLM for the earliest shift</h3>
            <p>
              Prompt the model with the paragraphs in <code>G<sub>i</sub></code> and ask it to return the <em>first</em> paragraph (not the very first in the group) where content clearly changes relative to what came before. Use that returned ID as the chunk boundary; start the next group at that paragraph and repeat to the end of the book.
            </p>

            <div class="algorithm-box">
              <strong>Step 3: Boundary Selection</strong><br>
              <code>boundary_id = LLM_earliest_shift(G)</code><br>
              <code>emit_chunk(start_id .. boundary_id-1)</code><br>
              <code>next_start = boundary_id</code>
            </div>

            <h3 class="title is-4">Choosing the context size (θ)</h3>
            <p>
              We sweep <code>θ ∈ [450, 1000]</code> tokens and find that <strong>θ ≈ 550</strong> consistently maximizes retrieval quality: large enough for context, small enough to keep the model focused on the current turn in the story.
            </p>

            <!-- <div class="notification is-info is-light">
              <strong>Include Figure 2 here (full-width):</strong> DCG@k across different <code>θ</code>, with <code>θ=550</code> highlighted as the sweet spot.
            </div> -->
            <div class="image-wrapper" style="display: flex; justify-content: center; align-items: center;">
            <img src="dcg token.png" alt="LumberChunker Pipeline" style="max-width: 80%; border-radius: 8px;">
            </div>
            <h3 class="title is-5">Why this works</h3>
            <p>
              Narratives work off of topic turns, scene changes, and discourse shifts, not uniform token distances. By explicitly locating the earliest meaningful change inside a window, LumberChunker produces variable-length chunks that keep entities and events intact, improving retrieval quality downstream.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Results Section: GutenQA -->
  <section class="section" id="gutenqa">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-left">
        <div class="column is-four-fifths">
          <h2 class="title is-3">GutenQA: Movie-style results, but for books</h2>
          <div class="content has-text-justified">

            <p>
              To evaluate chunking where it matters, we introduce <strong>GutenQA</strong>, a benchmark of <strong>100</strong> carefully cleaned public-domain books paired with <strong>3,000</strong> needle-in-a-haystack QA items (short, verifiable answers). This lets us measure <em>passage retrieval</em> precisely and then see how that lift translates into <em>RAG QA</em>.
            </p>

            <h3 class="title is-4">Retrieval: LumberChunker leads ⭐</h3>
            <p>
              Across DCG@k and Recall@k, LumberChunker ranks first. At <code>k=20</code>, it reaches <strong>DCG ≈ 62.1</strong> and <strong>Recall ≈ 77.9%</strong>, outperforming strong baselines like Recursive, Paragraph, Semantic, and Proposition chunking.
            </p>

            <div style="text-align: center; margin-bottom: 1rem;">
              <p style="font-weight: 600; font-size: 1.15rem; color: #464646; margin-bottom: 0.25rem;">Retrieval Performance Comparison</p>
              <p style="font-weight: 200; font-size: 1rem; color: #666;">NarrativeQA</p>
            </div>

            <!-- Custom Tab Interface -->
            <div class="custom-tabs">
              <ul>
                <li class="is-active" data-view="ndcg">DCG @ k</li>
                <li data-view="recall">Recall@k</li>
              </ul>
            </div>

            <!-- Table Card for DCG @ k -->
            <div id="table-container-ndcg" class="table-card">
              <table class="table is-bordered is-hoverable">
                <thead>
                  <tr>
                    <th></th>
                    <th>1</th>
                    <th>2</th>
                    <th>5</th>
                    <th>10</th>
                    <th>20</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th>Semantic Chunking</th>
                    <td>29.50</td>
                    <td>35.31</td>
                    <td>40.67</td>
                    <td>43.14</td>
                    <td>44.74</td>
                  </tr>
                  <tr>
                    <th>Paragraph-Level</th>
                    <td>36.54</td>
                    <td>42.11</td>
                    <td>45.87</td>
                    <td>47.72</td>
                    <td>49.00</td>
                  </tr>
                  <tr>
                    <th>Recursive Chunking</th>
                    <td>39.04</td>
                    <td>45.37</td>
                    <td>50.66</td>
                    <td>53.25</td>
                    <td>54.72</td>
                  </tr>
                  <tr>
                    <th>HyDE<sup>†</sup></th>
                    <td>33.47</td>
                    <td>39.74</td>
                    <td>45.06</td>
                    <td>48.14</td>
                    <td>49.92</td>
                  </tr>
                  <tr>
                    <th>Proposition-Level</th>
                    <td>36.91</td>
                    <td>42.42</td>
                    <td>44.88</td>
                    <td>45.65</td>
                    <td>46.19</td>
                  </tr>
                  <tr style="background-color: #f0f8ff;">
                    <th>LumberChunker</th>
                    <td><strong>48.28</strong></td>
                    <td><strong>54.86</strong></td>
                    <td><strong>59.37</strong></td>
                    <td><strong>60.99</strong></td>
                    <td><strong>62.09</strong></td>
                  </tr>
                </tbody>
              </table>
            </div>

            <!-- Table Card for Recall@k (initially hidden) -->
            <div id="table-container-recall" class="table-card" style="display: none;">
              <table class="table is-bordered is-hoverable">
                <thead>
                  <tr>
                    <th></th>
                    <th>1</th>
                    <th>2</th>
                    <th>5</th>
                    <th>10</th>
                    <th>20</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th>Semantic Chunking</th>
                    <td>29.50</td>
                    <td>38.70</td>
                    <td>50.60</td>
                    <td>58.21</td>
                    <td>64.51</td>
                  </tr>
                  <tr>
                    <th>Paragraph-Level</th>
                    <td>36.54</td>
                    <td>45.37</td>
                    <td>53.67</td>
                    <td>59.34</td>
                    <td>64.34</td>
                  </tr>
                  <tr>
                    <th>Recursive Chunking</th>
                    <td>39.04</td>
                    <td>49.07</td>
                    <td>60.64</td>
                    <td>68.62</td>
                    <td>74.35</td>
                  </tr>
                  <tr>
                    <th>HyDE<sup>†</sup></th>
                    <td>33.47</td>
                    <td>43.41</td>
                    <td>55.11</td>
                    <td>64.61</td>
                    <td>71.61</td>
                  </tr>
                  <tr>
                    <th>Proposition-Level</th>
                    <td>36.91</td>
                    <td>45.64</td>
                    <td>51.04</td>
                    <td>53.41</td>
                    <td>55.54</td>
                  </tr>
                  <tr style="background-color: #f0f8ff;">
                    <th>LumberChunker</th>
                    <td><strong>48.28</strong></td>
                    <td><strong>58.71</strong></td>
                    <td><strong>68.58</strong></td>
                    <td><strong>73.58</strong></td>
                    <td><strong>77.92</strong></td>
                  </tr>
                </tbody>
              </table>
            </div>

            <script>
              document.addEventListener('DOMContentLoaded', function() {
                const tabs = document.querySelectorAll('.custom-tabs ul li');
                const ndcgContainer = document.getElementById('table-container-ndcg');
                const recallContainer = document.getElementById('table-container-recall');
              
                tabs.forEach(tab => {
                  tab.addEventListener('click', () => {
                    // Remove active state from all tabs
                    tabs.forEach(t => t.classList.remove('is-active'));
                    // Activate the clicked tab
                    tab.classList.add('is-active');
              
                    // Toggle the table containers
                    if (tab.getAttribute('data-view') === 'ndcg') {
                      ndcgContainer.style.display = 'block';
                      recallContainer.style.display = 'none';
                    } else {
                      ndcgContainer.style.display = 'none';
                      recallContainer.style.display = 'block';
                    }
                  });
                });
              });
            </script>

            <h3 class="title is-4">Downstream QA: targeted retrieval beats giant context</h3>
            <p>
              Plugging chunks into a standard RAG pipeline (on autobiographies), <strong>RAG-LumberChunker</strong> surpasses <strong>RAG-Recursive</strong> and trails only <strong>RAG-Manual</strong> (hand-segmented ground truth). Notably, an “open-book” non-retrieval setting with huge context windows still underperforms RAG. (<em>targeted passages beat raw context size</em>.)
            </p>

            <!-- <div class="notification is-warning is-light">
              <strong>Include Figure 3 here (full-width):</strong> QA accuracy comparison (RAG-LumberChunker, RAG-Recursive, RAG-Manual, and non-RAG open-book).
            </div> -->

            <div class="image-wrapper" style="display: flex; justify-content: center; align-items: center;">
            <img src="qa acc.png" alt="LumberChunker Pipeline" style="max-width: 80%; border-radius: 8px;">
            </div>

            <h3 class="title is-4">Closer to human boundaries</h3>
            <p>
              Compared against manual chunks, LumberChunker achieves <strong>ROUGE-L ≈ 0.709</strong> vs. <strong>≈ 0.689</strong> for Recursive chunks, which is evidence that its boundaries align with how readers naturally perceive topic shifts.
            </p>

            <!-- Table 2: Similarity to manual segmentation (ROUGE-L) -->
            <div class="table-card" aria-labelledby="table2-caption">
              <table class="table is-bordered is-hoverable">
                <caption id="table2-caption">Table 2: Average ROUGE-L scores of methods compared to Manual Chunks.</caption>
                <thead>
                  <tr>
                    <th style="text-align: left;">Method</th>
                    <th>Average ROUGE-L Score</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align: left;">LumberChunker</td>
                    <td>0.709</td>
                  </tr>
                  <tr>
                    <td style="text-align: left;">Recursive Chunks</td>
                    <td>0.689</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <h3 class="title is-4">Granularity that “feels right”</h3>
            <p>
              Average chunk sizes (tokens) on the 100-book corpus: Paragraph ~79, Semantic ~185, Recursive ~399, <strong>LumberChunker ~334</strong>, Proposition ~12. Even with <code>θ=550</code>, the model frequently finds earlier shifts, yielding compact, on-topic chunks and reducing “lost-in-the-middle” effects.
            </p>

            <!-- Table 10: Average tokens and total chunks per method -->

            <!-- Accessible HTML table fallback (renders even if image is missing) -->
            <div class="table-card" aria-labelledby="table10-caption">
              <table class="table is-bordered is-hoverable">
                <caption id="table10-caption">Table 10: The average number of tokens per chunk and the total number of chunks after segmenting each book in the GutenQA.</caption>
                <thead>
                  <tr>
                    <th style="text-align: left;">Method</th>
                    <th>Avg. #Tokens / Chunk</th>
                    <th>Total #Chunks</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align: left;">Semantic Chunking</td>
                    <td>185 tokens</td>
                    <td>191059</td>
                  </tr>
                  <tr>
                    <td style="text-align: left;">Paragraph Level</td>
                    <td>79 tokens</td>
                    <td>248307</td>
                  </tr>
                  <tr>
                    <td style="text-align: left;">Recursive Chunking</td>
                    <td>399 tokens</td>
                    <td>31787</td>
                  </tr>
                  <tr>
                    <td style="text-align: left;">Proposition-Level</td>
                    <td>12 tokens</td>
                    <td>914493</td>
                  </tr>
                  <tr style="background-color: #f0f8ff;">
                    <td style="text-align: left;">LumberChunker</td>
                    <td><strong>334 tokens</strong></td>
                    <td><strong>36917</strong></td>
                  </tr>
                </tbody>
              </table>
            </div>

            <h3 class="title is-4">Practicality: predictable time &amp; cost ⚡</h3>
            <p>
              LumberChunker requires LLM calls, but costs scale linearly with book length and are one-time. Examples: <em>A Christmas Carol</em> (~710 paragraphs) ≈ <strong>95 s</strong>, ≈ <strong>$0.03</strong>; <em>The Count of Monte Cristo</em> (~14,339 paragraphs) ≈ <strong>1628 s</strong>, ≈ <strong>$0.40</strong> (pricing at paper time).
            </p>

            <div class="columns">
              <div class="column">
                <!-- Table 11: Runtime (seconds per book) -->

                <!-- Accessible HTML table fallback for Table 11 -->
                <div class="table-card" aria-labelledby="table11-caption">
                  <table class="table is-bordered is-hoverable">
                    <caption id="table11-caption">Table 11: The time required to apply LumberChunker or baselines on each book.</caption>
                    <thead>
                      <tr>
                        <th style="text-align: left;">Method</th>
                        <th>A Christmas Carol<br><span style="font-weight: 400;">(710 Paragraphs)</span></th>
                        <th>The Count of Monte Cristo<br><span style="font-weight: 400;">(14339 Paragraphs)</span></th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td style="text-align: left;">Semantic Chunking</td>
                        <td>212 seconds</td>
                        <td>4978 seconds</td>
                      </tr>
                      <tr>
                        <td style="text-align: left;">Recursive Chunking</td>
                        <td>0.1 seconds</td>
                        <td>0.6 seconds</td>
                      </tr>
                      <tr>
                        <td style="text-align: left;">HyDE</td>
                        <td>75 seconds</td>
                        <td>79 seconds</td>
                      </tr>
                      <tr>
                        <td style="text-align: left;">Proposition-Level</td>
                        <td>633 seconds</td>
                        <td>10302 seconds</td>
                      </tr>
                      <tr style="background-color: #f0f8ff;">
                        <td style="text-align: left;">LumberChunker</td>
                        <td><strong>95 seconds</strong></td>
                        <td><strong>1628 seconds</strong></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
              <div class="column">
                <!-- Table 12: Segmentation cost (per book) -->

                <!-- Accessible HTML table fallback for Table 12 -->
                <div class="table-card" aria-labelledby="table12-caption">
                  <table class="table is-bordered is-hoverable">
                    <caption id="table12-caption">Table 12: Cost comparison of segmenting two books with significantly different lengths.</caption>
                    <thead>
                      <tr>
                        <th></th>
                        <th>A Christmas Carol</th>
                        <th>The Count of Monte Cristo</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <th style="text-align: left;">Length</th>
                        <td>28k words</td>
                        <td>450k words</td>
                      </tr>
                      <tr>
                        <th style="text-align: left;">#LumberChunker Chunks</th>
                        <td>92</td>
                        <td>1476</td>
                      </tr>
                      <tr>
                        <th style="text-align: left;">Cost</th>
                        <td>$0.03</td>
                        <td>$0.40</td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
            </div>


          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Conclusion & Limitations Section -->
  <section class="section" style="margin-top: -2rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-left">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Conclusion & Limitations</h2>
          <div class="content has-text-justified">
            <p>
              LumberChunker shows that <strong>LLM-guided narrative segmentation</strong> can strike a rare balance of preserving story flow without demanding massive compute or retraining. 
              By letting an LLM detect <em>where meaning actually shifts</em> inside rolling paragraph windows, we obtain chunks that feel natural to humans and perform better for machines.
            </p>

            <p>
              On the <strong>GutenQA benchmark</strong>, LumberChunker consistently improves retrieval and downstream QA over traditional fixed-size and recursive methods,
              approaching the quality of manual, human-curated segmentations. Its efficiency, roughly linear in paragraph count, makes it practical for large-scale preprocessing in RAG pipelines.
            </p>

            <p>
              Still, there are tradeoffs worth noting:
            </p>
            <ul>
              <li>
                <strong>LLM dependency:</strong> Each segmentation call requires an LLM inference. While cost scales predictably with document length, it remains slower than rule-based or embedding-only approaches.
              </li>
              <li>
                <strong>Domain sensitivity:</strong> LumberChunker excels in long-form narratives, but structured or highly repetitive texts (legal, scientific, or instructional) may not benefit as much.
              </li>
              <li>
                <strong>Scaling limits:</strong> For extremely large corpora, iterative prompting adds latency, though the cost remains a one-time preprocessing step.
              </li>
            </ul>

            <p>
              These limitations also point toward future directions:
            </p>
            <ul>
              <li>Developing <strong>hybrid models</strong> that blend structural cues with learned semantic boundaries.</li>
              <li>Adapting thresholds dynamically based on <strong>document style and length</strong>.</li>
              <li>Exploring <strong>query-aware chunking</strong> to tailor segmentation directly to retrieval goals.</li>
            </ul>

            <p style="margin-top: 1.5rem;">
              As RAG systems continue to scale, <strong>effective document segmentation</strong> will remain a key frontier.
              LumberChunker offers a practical step forward—one that respects both meaning and efficiency, making
              long-form understanding more accessible to modern language models.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Citation Section -->
  <section class="section" style="background-color: #f8f9fa; margin-top: 2rem; padding: 2rem 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Citation</h2>
          <div class="content">
            <p>If you find LumberChunker useful in your research, please consider citing:</p>
            <pre style="background-color: #ffffff; padding: 1rem; border-radius: 5px; overflow-x: auto;"><code>@misc{duarte2024lumberchunker,
      title={LumberChunker: Long-Form Narrative Document Segmentation}, 
      author={André V. Duarte and João Marques and Miguel Graça and Miguel Freire and Lei Li and Arlindo L. Oliveira},
      year={2024},
      eprint={2406.17526},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17526}, 
}</code></pre>
          </div>
        </div>
      </div>
    </div>
  </section>


  <footer class="footer" style="padding: 2rem 0; background-color: #f8f9fa;">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
        <p>
          Website template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </p>
      </div>
    </div>
  </footer>

</body>
</html>
